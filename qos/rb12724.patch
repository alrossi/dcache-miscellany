diff --git a/packages/fhs/src/main/assembly/filter.properties b/packages/fhs/src/main/assembly/filter.properties
index 00f5a96f6ac9cb72674c9269a4b966463492c58b..10fb97a03792226c436bcf6e1d24a013a7907997 100644
--- a/packages/fhs/src/main/assembly/filter.properties
+++ b/packages/fhs/src/main/assembly/filter.properties
@@ -104,3 +104,4 @@ dcache.paths.bulk=/var/lib/dcache/bulk
 dcache.paths.resilience=/var/lib/dcache/resilience
 dcache.paths.pool-history=/var/lib/dcache/pool-history
 dcache.paths.nfs=/var/lib/dcache/nfs
+dcache.paths.qos=/var/lib/dcache/qos
diff --git a/packages/fhs/src/main/rpm/dcache-server.spec b/packages/fhs/src/main/rpm/dcache-server.spec
index 46ef848f43130e26e2f1eea81c23f95dbbb327e4..bbafdd7bfe0206968a069ee8cb06883646aabe46 100644
--- a/packages/fhs/src/main/rpm/dcache-server.spec
+++ b/packages/fhs/src/main/rpm/dcache-server.spec
@@ -118,6 +118,7 @@ rm -rf $RPM_BUILD_ROOT
 %attr(700,dcache,dcache) /var/lib/dcache/httpd
 %attr(700,dcache,dcache) /var/lib/dcache/pool-history
 %attr(700,dcache,dcache) /var/lib/dcache/resilience
+%attr(700,dcache,dcache) /var/lib/dcache/qos
 %attr(700,dcache,dcache) /var/lib/dcache/statistics
 %attr(700,dcache,dcache) /var/lib/dcache/nfs
 %attr(750,dcache,dcache) /var/lib/dcache/billing
diff --git a/packages/system-test/src/main/assembly/filter.properties b/packages/system-test/src/main/assembly/filter.properties
index 123eb4dc2b6861867404cfcd2e48205a3f12ae3d..dda8cfe1b4263ae61850700f490b3117b1e87bb4 100644
--- a/packages/system-test/src/main/assembly/filter.properties
+++ b/packages/system-test/src/main/assembly/filter.properties
@@ -69,3 +69,4 @@ dcache.paths.bulk=${dcache.home}/var/bulk
 dcache.paths.resilience=${dcache.home}/var/resilience
 dcache.paths.pool-history=${dcache.home}/var/pool-history
 dcache.paths.nfs=${dcache.home}/var/nfs
+dcache.paths.qos=${dcache.home}/var/qos
diff --git a/packages/tar/src/main/assembly/filter.properties b/packages/tar/src/main/assembly/filter.properties
index b60a31f584cc41f1c525dbae8cbbb286135e0dd0..992d2fbf9fd0c765d9e0b5a8aa71b08d07c20c7e 100644
--- a/packages/tar/src/main/assembly/filter.properties
+++ b/packages/tar/src/main/assembly/filter.properties
@@ -113,3 +113,4 @@ dcache.paths.alarms=${dcache.home}/var/alarms
 dcache.paths.bulk=${dcache.home}/var/bulk
 dcache.paths.resilience=${dcache.home}/var/resilience
 dcache.paths.nfs=${dcache.home}/var/nfs
+dcache.paths.qos=${dcache.home}/var/qos
diff --git a/skel/etc/logback.xml b/skel/etc/logback.xml
index dfb1edd84828c51847062cb4a3acf29ac090ced0..722c1f6ead952abd3cea250632b4717beaef9836 100644
--- a/skel/etc/logback.xml
+++ b/skel/etc/logback.xml
@@ -150,12 +150,11 @@
     </then>
   </if>
 
-
-  <appender name="resilience" class="ch.qos.logback.core.rolling.RollingFileAppender">
-    <file>${dcache.log.dir}/${dcache.domain.name}.resilience</file>
+  <appender name="qos" class="ch.qos.logback.core.rolling.RollingFileAppender">
+    <file>${dcache.log.dir}/${dcache.domain.name}.qos</file>
     <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
-      <fileNamePattern>${dcache.log.dir}/${dcache.domain.name}.resilience.%d{yyyy-MM-dd}.gz</fileNamePattern>
-      <maxHistory>${dcache.log.resilience.max-history}</maxHistory>
+      <fileNamePattern>${dcache.log.dir}/${dcache.domain.name}.qos.%d{yyyy-MM-dd}.gz</fileNamePattern>
+      <maxHistory>${dcache.log.qos.max-history}</maxHistory>
     </rollingPolicy>
     <encoder>
       <pattern>${dcache.log.format.file}</pattern>
@@ -227,8 +226,13 @@
     <appender-ref ref="zookeeper"/>
   </logger>
 
+  <logger name="org.dcache.qos-log" additivity="false">
+    <appender-ref ref="qos"/>
+  </logger>
+
+  <!-- for backward compatibility, deprecated by qos -->
   <logger name="org.dcache.resilience-log" additivity="false">
-    <appender-ref ref="resilience"/>
+    <appender-ref ref="qos"/>
   </logger>
 
   <!-- Nothing is logged to this logger. Its sole purpose is to list
@@ -250,7 +254,7 @@
         <appender-ref ref="kafka" />
       </then>
     </if>
-    <appender-ref ref="resilience"/>
+    <appender-ref ref="qos"/>
   </logger>
 
   <turboFilter class="dmg.util.logback.CellThresholdFilter">
@@ -359,7 +363,14 @@
     </threshold>
 
     <threshold>
-      <appender>resilience</appender>
+      <appender>qos</appender>
+      <logger>org.dcache.qos-log</logger>
+      <level>${dcache.log.level.qos}</level>
+    </threshold>
+
+    <!-- For backward compatibility -->
+    <threshold>
+      <appender>qos</appender>
       <logger>org.dcache.resilience-log</logger>
       <level>${dcache.log.level.resilience}</level>
     </threshold>
diff --git a/skel/share/defaults/dcache.properties b/skel/share/defaults/dcache.properties
index b712b4101c56c3f02e7acdd56d06eefd1c9fe68c..cc0710949bd974152a42d0778af0669c8e4aeb49 100644
--- a/skel/share/defaults/dcache.properties
+++ b/skel/share/defaults/dcache.properties
@@ -73,6 +73,7 @@
 (not-for-services,one-of?off|error|warn|info|debug|trace|all)dcache.log.level.access=info
 (not-for-services,one-of?off|error|warn|info|debug|trace|all)dcache.log.level.zookeeper=info
 (not-for-services,one-of?off|error|warn|info|debug|trace|all)dcache.log.level.resilience=error
+(not-for-services,one-of?off|error|warn|info|debug|trace|all)dcache.log.level.qos=error
 
 # How many days to keep access logs
 dcache.log.access.max-history=30
@@ -83,6 +84,9 @@ dcache.log.zookeeper.max-history=30
 # How many days to keep resilience logs
 dcache.log.resilience.max-history=30
 
+# How many days to keep qos logs
+dcache.log.qos.max-history=30
+
 # Host on which the remote log server will run
 # relative to this dCache installation
 #
@@ -302,6 +306,10 @@ dcache.pool-monitor.update-period.unit = SECONDS
 #
 dcache.pool-monitor.max-updates-per-second = 2.0
 
+# Topic to subscribe to for messages concerning the completion of transitions
+# to tape (flush) or to disk (stage).
+dcache.qos-transition.topic=QoSTransitionTopic
+
 #  -----------------------------------------------------------------------
 #     Common network related parameters
 #  -----------------------------------------------------------------------
@@ -519,6 +527,7 @@ dcache.service.srmmanager = ${dcache.queue.srmmanager}
 dcache.service.history = history
 dcache.service.bulk = bulk
 dcache.service.ping = ping
+dcache.service.qos = qos-engine
 
 
 #  -----------------------------------------------------------------------
diff --git a/skel/share/defaults/paths.properties b/skel/share/defaults/paths.properties
index ef7d6ab37cf4f85ea9b1b7cb3ff00f5fa516b39e..e4d3a6cb16d61e11a023c1d26fd57a033693bc45 100644
--- a/skel/share/defaults/paths.properties
+++ b/skel/share/defaults/paths.properties
@@ -50,3 +50,4 @@ dcache.paths.content-types = ${dcache.paths.etc}/content-types.properties
 dcache.paths.grid-security = /etc/grid-security
 
 dcache.paths.nfs=@dcache.paths.nfs@
+dcache.paths.qos=@dcache.paths.qos@
diff --git a/skel/share/defaults/pool.properties b/skel/share/defaults/pool.properties
index 7fcca74033f0d0dab4aeb35c0739d81d60b48282..defc42c649da758646afe7f6b285314dbd175525 100644
--- a/skel/share/defaults/pool.properties
+++ b/skel/share/defaults/pool.properties
@@ -598,7 +598,7 @@ pool.destination.replicate =
 pool.destination.replicate.ip=
 
 ##
-#  ------------------------------ Resilience ---------------------------------
+#  ------------------------------ Resilience/QoS ---------------------------------
 ##
 
 # Cell address to which to send corrupt file messages
@@ -609,6 +609,9 @@ pool.destination.corrupt-file=CorruptFileTopic
 #
 pool.resilience.request-threads=10
 
+#  ---- Number of threads processing request coming from the qos system.
+#
+pool.qos.request-threads=10
 
 ## ---- Number of threads processing requests coming from frontend services.
 #
diff --git a/skel/share/defaults/qos.properties b/skel/share/defaults/qos.properties
new file mode 100644
index 0000000000000000000000000000000000000000..3e57f6d9c37630a43d6fdd27b0b4ed5e03ef6fc5
--- /dev/null
+++ b/skel/share/defaults/qos.properties
@@ -0,0 +1,374 @@
+#  -----------------------------------------------------------------------
+#     Default values for qos
+#
+#     The qos services are responsible for maintaining the disk and tape
+#     requirements of a given file.
+#  -----------------------------------------------------------------------
+@DEFAULTS_HEADER@
+
+#  ---- Cell names of qos services
+#
+qos.cell.name=qos
+qos.engine.cell.name=qos-engine
+qos.verifier.cell.name=qos-verifier
+qos.scanner.cell.name=qos-scanner
+qos.adjuster.cell.name=qos-adjuster
+
+#  ---- Named queues to consume from
+#
+#       A service can consume messages from named queues. Other services can
+#       write messages to such queues. A named queue has an unqualified cell
+#       address, that is, an address without a domain name.
+#
+#       This property contains a comma separated list of named queues to
+#       consume from.
+#
+qos.cell.consume = ${qos.cell.name}
+qos.engine.cell.consume = ${qos.engine.cell.name}
+qos.verifier.cell.consume = ${qos.verifier.cell.name}
+qos.scanner.cell.consume = ${qos.scanner.cell.name}
+qos.adjuster.cell.consume = ${qos.adjuster.cell.name}
+
+#  ---- Message topics to subscribe to.
+#
+qos.cell.subscribe=${qos.cache-location-topic},\
+  ${qos.corrupt-file-topic},\
+  ${qos.pool-monitor-topic}
+
+qos.engine.cell.subscribe=${qos.cache-location-topic},\
+  ${qos.corrupt-file-topic},\
+  ${qos.pool-monitor-topic}
+
+qos.verifier.cell.subscribe=${qos.pool-monitor-topic}
+
+qos.scanner.cell.subscribe=${qos.pool-monitor-topic}
+
+# ---- Listens for location updates from PnfsManager.
+#
+qos.cache-location-topic=CacheLocationTopic
+
+# ---- Listens for checksum scanner or pool reports.  If the corrupt
+#      file is a non-unique replica, it tries to handle this by removing
+#      the copy and making a new one.
+#
+qos.corrupt-file-topic=${dcache.corrupt-file.topic}
+
+# ---- Channel on which pool monitor updates are pushed out.
+#      Resilience relies on these for current info regarding pools,
+#      pool groups, storage units, pool mode/status, pool tags, and pool cost.
+#
+qos.pool-monitor-topic=${dcache.pool-monitor.topic}
+
+# ---- Publishes transition completed messages on this topic.
+#
+qos.transition-completed-topic=${dcache.qos-transition.topic}
+
+# ---- Base directory where any qos metadata is stored.  This
+#      includes the checkpoint file, inaccessible file lists, and statistics
+#      output.
+#
+qos.home=${dcache.paths.qos}
+
+# ---- Configuration for database connection pool ---------------------------
+#
+#      The database connection pool reuses connections between successive
+#      database operations.  By reusing connections dCache doesn't suffer
+#      the overhead of establishing new database connections for each
+#      operation.
+#
+#      The options here determine how qos behaves as the number of concurrent
+#      requests fluctuates.
+# ---------------------------------------------------------------------------
+
+# ---- The maximum number of concurrent database connections
+#
+#      The recommended minimum setting is the number of scan threads
+#      plus a few more for admin calls.
+#
+#      Since the scanner service shares the chimera database with pnfsmanager,
+#      be sure to adjust the postgresql.conf max connections upwards
+#      to accommodate both.  Pnfsmanager runs well with about 100
+#      connections.  Adding a separate qos service means the
+#      connections should be increased by at least the amount below.
+#
+qos.db.connections.max=10
+
+# ---- The minimum number of idle database connections.
+#
+qos.db.connections.idle=1
+
+(prefix)qos.db.hikari-properties = Hikari-specific properties
+
+# ---- Database related settings reserved for internal use.
+#
+(immutable)qos.db.host=${chimera.db.host}
+(immutable)qos.db.name=${chimera.db.name}
+(immutable)qos.db.user=${chimera.db.user}
+(immutable)qos.db.password=${chimera.db.password}
+(immutable)qos.db.password.file=${chimera.db.password.file}
+(immutable)qos.db.url=${chimera.db.url}
+(immutable)qos.db.schema.changelog=${chimera.db.schema.changelog}
+(immutable)qos.db.schema.auto=false
+
+# ---- Used with the pool scan query. This is a hint given to the jdbc driver
+#      to decrease the number of round-trips to the database on large result
+#      sets (by default it is 0, meaning ignored).  Setting this too high
+#      may, however, adversely affect performance.
+#
+qos.db.fetch-size=1000
+
+# ---- Replace with org.dcache.chimera.namespace.ChimeraEnstoreStorageInfoExtractor
+#      if you are running an enstore HSM backend.
+#
+qos.plugins.storage-info-extractor=${dcache.plugins.storage-info-extractor}
+
+# ---- Checkpointing.
+#
+#      How often the verification operation table is to be saved to disk for
+#      the purposes of recovery.
+#
+qos.limits.checkpoint-expiry=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.checkpoint-expiry.unit=MINUTES
+
+# ---- Thread queues --------------------------------------------------------------
+#
+#      There are different thread queues associated with each of the qos services.
+#
+#      In general, each (remote) service has an executor for handling the
+#      processing of incoming messages.  The thread pools for these
+#      are labeled 'submit-threads.'  In the case of the verifier,
+#      there is also a bulk submission pool for handling bulk scan requests.
+#
+#      The verifier, scanner and adjuster in addition also have task thread pools.
+# ---------------------------------------------------------------------------------
+
+# ---- Thread queue used during the request for and update to requirements.
+#      In the current implementation, each thread makes a call to the namespace.
+#
+qos.limits.requirements.submit-threads=32
+
+# ---- Thread queue used during the registration of a new operation.
+#      Updates will do a verification of the pool locations; cancellation
+#      also runs on these threads, but involves simply setting up a filter
+#      (the actual cancellation is run on a different thread).
+#
+qos.limits.verifier.submit-threads=32
+
+# ---- Thread queue used during the registration of a new operation.
+#      Like the above, but loops over a list of files.
+#
+qos.limits.verifier.bulk-threads=8
+
+# ---- Thread queue used when an operation becomes active (to verify
+#      the requirements and send a message to the adjuster). Each thread makes
+#      a call to the requirements service, then to the pools, and finally,
+#      if necessary, to the adjuster service.
+#
+qos.limits.verifier.task-threads=32
+
+# ---- Thread queue used during the registration of a new adjustment task.
+#      Minimal work is done on this thread.
+#
+qos.limits.adjuster.submit-threads=16
+
+# ---- Thread queue used for tasks.  Note that the longer-running tasks like
+#      like migration and staging relinquish the thread by waiting.
+#
+qos.limits.adjuster.task-threads=100
+
+# ---- Thread queue used to handle responses from the verifier.  These
+#      involve batched counts, and the amount of update work done on the
+#      thread is small.  Should mirror the bulk threads on the verifier.
+#
+qos.limits.scanner.submit-threads=8
+
+# ---- Thread queue used for scanning the namespace on pool state changes or
+#      as part of a periodic check.  Requires a database connection,
+#      which it holds onto for the life of the task being executed.
+#
+#      A note on pool operation throttling:
+#
+#      A pool scan or processing of a pool status message can generate
+#      thousands, even millions, of file tasks.  Allowing too many pool
+#      operations to run simultaneously can, aside from the increased
+#      pressure on the namespace database, potentially overload the system.
+#      Lowering the number of available threads may be necessary
+#      if the number of files per pool is on the order of 2 million or
+#      greater (or, alternately, one may need to increase the memory of the
+#      JVM for the scanner service).
+#
+qos.limits.scanner.task-threads=5
+
+qos.limits.scanner.pool-op-init-grace-period=5
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.pool-op-init-grace-period.unit=MINUTES
+
+# ---- The number of verification operations needs to be bounded so as not
+#      to flood either the adjuster service or the pin manager (for staging).
+#      This number need not, however, reflect the number of available task threads
+#      in the adjuster itself.
+#
+#      A note on the memory footprint.  Each verification operation entry
+#      requires 128 bytes minimum (assuming 8-byte alignment) of primitives or
+#      object references. If we assume each reference could encompass 512 bytes,
+#      then 10 million entries would require about 5 GB.  However, it is
+#      advisable to set the verifier domain heap size to at least 8 GB,
+#      because there is other overhead during normal processing which
+#      can drive up peak usage values.  Naturally, if the service runs
+#      together with other services in the JVM, the heap size should also be
+#      adjusted upward.
+#
+(immutable)qos.limits.verifier.max-running-operations=200
+
+# ---- So that long-lived staging requests do no starve out other kinds of
+#      requests, we only allow this proportion of concurrently running operations
+#      to be staging.
+qos.limits.verifier.staging-max-allocation=0.5
+
+# ---- Size of buffer for displaying history of the most
+#      recently completed tasks and operations.
+#
+qos.limits.adjuster.task-history=1000
+qos.limits.verifier.operation-history=1000
+
+# ---- Retry management.
+#
+#      The following property controls the number of
+#      times the verifier is allowed to retry failed file-operations.
+#      This is on a per-source/target basis, if the error is judged retriable.
+#      If there is a non-retriable error, but a different source or target
+#      can be selected, the retry count is set back to 0 again.
+#
+qos.limits.verifier.operation-retries=1
+
+# ---- Retry management.
+#
+#      The following property controls the number of
+#      times the adjuster is allowed to retry failed adjustment tasks.
+#
+qos.limits.adjuster.task-retries=1
+
+# ---- Operation and task map checking.
+#
+#      The maximum interval which can pass before a check of waiting/completed
+#      operations or tasks is run (for an active system the interval will effectively
+#      be shorter, as checks are also done each time a running task terminates).
+#
+qos.limits.verifier.scan-period=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.verifier.scan-period.unit=MINUTES
+qos.limits.adjuster.scan-period=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.adjuster.scan-period.unit=MINUTES
+
+# ---- Pool manager pool info refreshing.
+#
+#      Information concerning pool cost is considered out of sync after
+#      this interval has passed.   This should be somewhat longer than
+#      the notification period value(see poolmanager.pool-monitor.update-period and
+#      poolmanager.pool-monitor.update-period.unit).
+#
+qos.limits.pool-info-expiry=3
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.pool-info-expiry.unit=MINUTES
+
+# ---- Pool Status update handling.
+#
+#      How long to wait between the reception of a pool down update
+#      and actually launching a scan operation to check replicas on
+#      that pool.  Setting to 0 will trigger the scan immediately.
+#
+qos.limits.scanner.down-grace-period=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.down-grace-period.unit=HOURS
+
+# ---- Pool Status update handling.
+#
+#      How long to wait between the reception of a pool restart update
+#      and actually launching a scan operation to check replicas on
+#      that pool. Setting to 0 will trigger the scan immediately.
+#
+qos.limits.scanner.restart-grace-period=6
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.restart-grace-period.unit=HOURS
+
+# ---- Startup
+#
+#      When an entire dcache installation is brought on line at the same time,
+#      pool status may not yet be available from the pool manager.  This
+#      property sets an initial delay before pool info initialization
+#      begins.  Setting this property to 0 skips the delay.
+#
+qos.limits.startup-delay=30
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.startup-delay.unit=SECONDS
+
+# ---- Operation launch
+#
+#      Scheduling the operation can include a delay between when it has been selected to run
+#      and when it actually starts to execute.  This may be useful in some situations
+#      to avoid races between PnfsManager and qos in terms of the initial
+#      write to a pool.  This property can also be configured from the admin
+#      interface using file ctrl reset.
+#
+qos.limits.verification.launch-delay=0
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.verification.launch-delay.unit=SECONDS
+
+# ---- Copy/migration target selection.
+#
+#      Strategy implementation used to select among available/eligible target pools.
+#
+qos.pool-selection-strategy=org.dcache.pool.migration.ProportionalPoolSelectionStrategy
+
+# ---- Periodic scanning (watchdog).
+#
+#      The following properties control the periodic scanning of pools to check
+#      for qos consistency and initiate any adjustments that may be necessary
+#      in the case of inconsistent state.  The scan period refers to the default
+#      amount of time between sweeps of the pools (absent pool status change events).
+#      The scan window refers to the maximum amount of time that can elapse since
+#      a pool was scanned after which another scan of that specific pool will be
+#      initiated. Disabling the watchdog means the system will only respond to actual
+#      ool state change events, but will not automatically scan pools after the window
+#      period has elapsed.
+#
+(one-of?true|false)qos.enable.watchdog=true
+qos.watchdog.scan.period=3
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.watchdog.scan.period.unit=MINUTES
+qos.watchdog.scan.window=24
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.watchdog.scan.window.unit=HOURS
+
+# ---- Endpoint (cell) settings for contacting pin manager.
+#
+qos.service.pinmanager=${dcache.service.pinmanager}
+qos.service.pinmanager.timeout=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pinmanager.timeout.unit=MINUTES
+
+# ---- Endpoint (cell) settings for contacting pnfs manager.
+#
+qos.service.pnfsmanager=${dcache.service.pnfsmanager}
+qos.service.pnfsmanager.timeout=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pnfsmanager.timeout.unit=MINUTES
+
+# ---- Endpoint (cell) settings for contacting pools (destination is dynamic).
+#
+qos.service.pool.timeout=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pool.timeout.unit=MINUTES
+
+# ---- Endpoint (cell) settings for the qos.transition-completed-topic.
+#
+qos.service.transition.timeout=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.transition.timeout.unit=MINUTES
+
+# ---- Main external entry point for qos.
+#
+qos.service.requirements=${dcache.service.qos}
+qos.service.requirements.timeout=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.requirements.timeout.unit=MINUTES
+
+# ---- Internal endpoints consumed only by other qos services.
+#
+(immutable)qos.service.adjustment=${qos.adjuster.cell.name}
+qos.service.adjustment.timeout=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.adjustment.timeout.unit=MINUTES
+
+(immutable)qos.service.scanner=${qos.scanner.cell.name}
+qos.service.scanner.timeout=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.scanner.timeout.unit=MINUTES
+
+(immutable)qos.service.verification=${qos.verifier.cell.name}
+qos.service.verification.timeout=1
+(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.verification.timeout.unit=MINUTES
diff --git a/skel/share/services/pool.batch b/skel/share/services/pool.batch
index b8f5ea27e071f7dfe0447d71054ac42765ea03e4..b62bd5d77671f7474a4ad2fc94a3c8610d630f71 100644
--- a/skel/share/services/pool.batch
+++ b/skel/share/services/pool.batch
@@ -75,6 +75,7 @@ check pool.destination.replicate
 check pool.destination.replicate.ip
 check pool.check-health-command
 check pool.resilience.request-threads
+check pool.qos.request-threads
 check pool.info-request-handler.threads
 
 define context PoolDefaults endDefine
diff --git a/skel/share/services/qos-adjuster.batch b/skel/share/services/qos-adjuster.batch
new file mode 100644
index 0000000000000000000000000000000000000000..28d18e17a7244ef2370b13f06b7df2892e8e60f0
--- /dev/null
+++ b/skel/share/services/qos-adjuster.batch
@@ -0,0 +1,26 @@
+## QoS services
+
+onerror shutdown
+check -strong qos.adjuster.cell.name
+check -strong qos.adjuster.cell.consume
+check -strong qos.transition-completed-topic
+check -strong qos.home
+check -strong qos.limits.adjuster.submit-threads
+check -strong qos.limits.adjuster.task-threads
+check -strong qos.limits.adjuster.task-history
+check -strong qos.limits.adjuster.task-retries
+check -strong qos.limits.adjuster.scan-period
+check -strong qos.limits.adjuster.scan-period.unit
+check -strong qos.service.pinmanager
+check -strong qos.service.pinmanager.timeout
+check -strong qos.service.pinmanager.timeout.unit
+check -strong qos.service.pool.timeout
+check -strong qos.service.pool.timeout.unit
+check -strong qos.service.verification
+check -strong qos.service.verification.timeout
+check -strong qos.service.verification.timeout.unit
+
+create org.dcache.cells.UniversalSpringCell ${qos.adjuster.cell.name} \
+        "classpath:org/dcache/qos/qos-adjuster.xml \
+        -subscribe=${qos.adjuster.cell.subscribe} \
+        -consume=${qos.adjuster.cell.consume}"
diff --git a/skel/share/services/qos-engine.batch b/skel/share/services/qos-engine.batch
new file mode 100644
index 0000000000000000000000000000000000000000..62608ba81a5085b155f7924b86235600a23be96d
--- /dev/null
+++ b/skel/share/services/qos-engine.batch
@@ -0,0 +1,26 @@
+## QoS services
+
+onerror shutdown
+check -strong qos.engine.cell.name
+check -strong qos.engine.cell.consume
+check -strong qos.engine.cell.subscribe
+check -strong qos.verifier.cell.subscribe
+check -strong qos.scanner.cell.subscribe
+check -strong qos.cache-location-topic
+check -strong qos.corrupt-file-topic
+check -strong qos.transition-completed-topic
+check -strong qos.home
+check -strong qos.limits.requirements.submit-threads
+check -strong qos.service.pnfsmanager
+check -strong qos.service.pnfsmanager.timeout
+check -strong qos.service.pnfsmanager.timeout.unit
+check -strong qos.service.transition.timeout
+check -strong qos.service.transition.timeout.unit
+check -strong qos.service.verification
+check -strong qos.service.verification.timeout
+check -strong qos.service.verification.timeout.unit
+
+create org.dcache.cells.UniversalSpringCell ${qos.engine.cell.name} \
+        "classpath:org/dcache/qos/qos-engine.xml \
+        -subscribe=${qos.engine.cell.subscribe} \
+        -consume=${qos.engine.cell.consume}"
\ No newline at end of file
diff --git a/skel/share/services/qos-scanner.batch b/skel/share/services/qos-scanner.batch
new file mode 100644
index 0000000000000000000000000000000000000000..548073563c84f113137454980a3ac0bf88169693
--- /dev/null
+++ b/skel/share/services/qos-scanner.batch
@@ -0,0 +1,40 @@
+## QoS scanner service
+
+onerror shutdown
+check -strong qos.enable.watchdog
+check -strong qos.scanner.cell.name
+check -strong qos.scanner.cell.consume
+check -strong qos.scanner.cell.subscribe
+check -strong qos.pool-monitor-topic
+check -strong qos.home
+check -strong qos.db.connections.max
+check -strong qos.db.connections.idle
+check -strong qos.db.fetch-size
+check -strong qos.plugins.storage-info-extractor
+check -strong qos.limits.scanner.submit-threads
+check -strong qos.limits.scanner.task-threads
+check -strong qos.limits.pool-info-expiry
+check -strong qos.limits.pool-info-expiry.unit
+check -strong qos.limits.scanner.down-grace-period
+check -strong qos.limits.scanner.down-grace-period.unit
+check -strong qos.limits.scanner.restart-grace-period
+check -strong qos.limits.scanner.restart-grace-period.unit
+check -strong qos.limits.scanner.pool-op-init-grace-period
+check -strong qos.limits.scanner.pool-op-init-grace-period.unit
+check -strong qos.limits.startup-delay
+check -strong qos.limits.startup-delay.unit
+check -strong qos.watchdog.scan.period
+check -strong qos.watchdog.scan.period.unit
+check -strong qos.watchdog.scan.window
+check -strong qos.watchdog.scan.window.unit
+check -strong qos.service.scanner
+check -strong qos.service.scanner.timeout
+check -strong qos.service.scanner.timeout.unit
+check -strong qos.service.verification
+check -strong qos.service.verification.timeout
+check -strong qos.service.verification.timeout.unit
+
+create org.dcache.cells.UniversalSpringCell ${qos.scanner.cell.name} \
+        "classpath:org/dcache/qos/qos-scanner.xml \
+        -subscribe=${qos.scanner.cell.subscribe} \
+        -consume=${qos.scanner.cell.consume}"
\ No newline at end of file
diff --git a/skel/share/services/qos-verifier.batch b/skel/share/services/qos-verifier.batch
new file mode 100644
index 0000000000000000000000000000000000000000..91426c146fd87597bd79690a3c76b6933735ef72
--- /dev/null
+++ b/skel/share/services/qos-verifier.batch
@@ -0,0 +1,46 @@
+## QoS verifier service
+
+onerror shutdown
+
+check -strong qos.verifier.cell.name
+check -strong qos.verifier.cell.consume
+check -strong qos.verifier.cell.subscribe
+check -strong qos.pool-monitor-topic
+check -strong qos.home
+check -strong qos.limits.checkpoint-expiry
+check -strong qos.limits.checkpoint-expiry.unit
+check -strong qos.limits.verifier.submit-threads
+check -strong qos.limits.verifier.bulk-threads
+check -strong qos.limits.verifier.task-threads
+check -strong qos.limits.verifier.max-running-operations
+check -strong qos.limits.verifier.staging-max-allocation
+check -strong qos.limits.verifier.operation-history
+check -strong qos.limits.verifier.operation-retries
+check -strong qos.limits.verifier.scan-period
+check -strong qos.limits.verifier.scan-period.unit
+check -strong qos.limits.pool-info-expiry
+check -strong qos.limits.pool-info-expiry.unit
+check -strong qos.limits.startup-delay
+check -strong qos.limits.startup-delay.unit
+check -strong qos.limits.verification.launch-delay
+check -strong qos.limits.verification.launch-delay.unit
+check -strong qos.pool-selection-strategy
+check -strong qos.service.pnfsmanager
+check -strong qos.service.pnfsmanager.timeout
+check -strong qos.service.pnfsmanager.timeout.unit
+check -strong qos.service.pool.timeout
+check -strong qos.service.pool.timeout.unit
+check -strong qos.service.requirements
+check -strong qos.service.requirements.timeout
+check -strong qos.service.requirements.timeout.unit
+check -strong qos.service.adjustment
+check -strong qos.service.adjustment.timeout
+check -strong qos.service.adjustment.timeout.unit
+check -strong qos.service.scanner
+check -strong qos.service.scanner.timeout
+check -strong qos.service.scanner.timeout.unit
+
+create org.dcache.cells.UniversalSpringCell ${qos.verifier.cell.name} \
+        "classpath:org/dcache/qos/qos-verifier.xml \
+        -subscribe=${qos.verifier.cell.subscribe} \
+        -consume=${qos.verifier.cell.consume}"
diff --git a/skel/share/services/qos.batch b/skel/share/services/qos.batch
new file mode 100644
index 0000000000000000000000000000000000000000..b0f9e1be81f52a65ce311fa2a36667795d0376fc
--- /dev/null
+++ b/skel/share/services/qos.batch
@@ -0,0 +1,80 @@
+## QoS services
+
+onerror shutdown
+check -strong qos.enable.watchdog
+check -strong qos.cell.name
+check -strong qos.cell.consume
+check -strong qos.cell.subscribe
+check -strong qos.cache-location-topic
+check -strong qos.corrupt-file-topic
+check -strong qos.pool-monitor-topic
+check -strong qos.transition-completed-topic
+check -strong qos.home
+check -strong qos.db.connections.max
+check -strong qos.db.connections.idle
+check -strong qos.db.fetch-size
+check -strong qos.plugins.storage-info-extractor
+check -strong qos.limits.checkpoint-expiry
+check -strong qos.limits.checkpoint-expiry.unit
+check -strong qos.limits.requirements.submit-threads
+check -strong qos.limits.verifier.submit-threads
+check -strong qos.limits.verifier.bulk-threads
+check -strong qos.limits.verifier.task-threads
+check -strong qos.limits.adjuster.submit-threads
+check -strong qos.limits.adjuster.task-threads
+check -strong qos.limits.scanner.submit-threads
+check -strong qos.limits.scanner.task-threads
+check -strong qos.limits.verifier.max-running-operations
+check -strong qos.limits.verifier.staging-max-allocation
+check -strong qos.limits.adjuster.task-history
+check -strong qos.limits.verifier.operation-history
+check -strong qos.limits.verifier.operation-retries
+check -strong qos.limits.adjuster.task-retries
+check -strong qos.limits.verifier.scan-period
+check -strong qos.limits.verifier.scan-period.unit
+check -strong qos.limits.adjuster.scan-period
+check -strong qos.limits.adjuster.scan-period.unit
+check -strong qos.limits.pool-info-expiry
+check -strong qos.limits.pool-info-expiry.unit
+check -strong qos.limits.scanner.down-grace-period
+check -strong qos.limits.scanner.down-grace-period.unit
+check -strong qos.limits.scanner.restart-grace-period
+check -strong qos.limits.scanner.restart-grace-period.unit
+check -strong qos.limits.scanner.pool-op-init-grace-period
+check -strong qos.limits.scanner.pool-op-init-grace-period.unit
+check -strong qos.limits.startup-delay
+check -strong qos.limits.startup-delay.unit
+check -strong qos.limits.verification.launch-delay
+check -strong qos.limits.verification.launch-delay.unit
+check -strong qos.pool-selection-strategy
+check -strong qos.watchdog.scan.period
+check -strong qos.watchdog.scan.period.unit
+check -strong qos.watchdog.scan.window
+check -strong qos.watchdog.scan.window.unit
+check -strong qos.service.pinmanager
+check -strong qos.service.pinmanager.timeout
+check -strong qos.service.pinmanager.timeout.unit
+check -strong qos.service.pnfsmanager
+check -strong qos.service.pnfsmanager.timeout
+check -strong qos.service.pnfsmanager.timeout.unit
+check -strong qos.service.pool.timeout
+check -strong qos.service.pool.timeout.unit
+check -strong qos.service.transition.timeout
+check -strong qos.service.transition.timeout.unit
+check -strong qos.service.requirements
+check -strong qos.service.requirements.timeout
+check -strong qos.service.requirements.timeout.unit
+check -strong qos.service.adjustment
+check -strong qos.service.adjustment.timeout
+check -strong qos.service.adjustment.timeout.unit
+check -strong qos.service.scanner
+check -strong qos.service.scanner.timeout
+check -strong qos.service.scanner.timeout.unit
+check -strong qos.service.verification
+check -strong qos.service.verification.timeout
+check -strong qos.service.verification.timeout.unit
+
+create org.dcache.cells.UniversalSpringCell ${qos.cell.name} \
+        "classpath:org/dcache/qos/qos.xml \
+        -subscribe=${qos.cell.subscribe} \
+        -consume=${qos.cell.consume}"
diff --git a/skel/var/qos/.empty-dir b/skel/var/qos/.empty-dir
new file mode 100644
index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391
